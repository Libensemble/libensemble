{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Optimization with APOSMM\n",
    "\n",
    "This tutorial demonstrates libEnsemble’s capability to identify multiple minima of simulation output using the built-in APOSMM (Asynchronously Parallel Optimization Solver for finding Multiple Minima) generator function (`gen_f`). In this tutorial, we’ll create a simple simulation function (`sim_f`) that defines a function with multiple minima, then write a libEnsemble calling script that imports APOSMM and parameterizes it to check for minima over a domain of outputs from our `sim_f`.\n",
    "\n",
    "Besides libEnsemble and NumPy, SciPy and mpmath are also required dependencies.\n",
    "\n",
    "**Note that for notebooks** the multiprocessing start method should be set to `fork` (default on Linux). To use\n",
    "with `spawn` (default on Windows and macOS), use the `multiprocess` library.\n",
    "\n",
    "Let's make sure libEnsemble is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install libensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Six-Hump Camel Simulation Function\n",
    "\n",
    "Describing APOSMM’s operations is simpler with a given function on which to depict evaluations. We’ll use the Six-Hump Camel function, known to have six global minima. A sample space of this function, containing all minima, appears below:\n",
    "\n",
    "![6humpcamel](https://raw.githubusercontent.com/Libensemble/libensemble/main/docs/images/basic_6hc.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our simulation function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def six_hump_camel(H, persis_info, sim_specs, _):\n",
    "    \"\"\"Six-Hump Camel sim_f.\"\"\"\n",
    "\n",
    "    batch = len(H[\"x\"])  # Num evaluations each sim_f call.\n",
    "    H_o = np.zeros(batch, dtype=sim_specs[\"out\"])  # Define output array H\n",
    "\n",
    "    for i, x in enumerate(H[\"x\"]):\n",
    "        H_o[\"f\"][i] = six_hump_camel_func(x)  # Function evaluations placed into H\n",
    "\n",
    "    return H_o, persis_info\n",
    "\n",
    "\n",
    "def six_hump_camel_func(x):\n",
    "    \"\"\"Six-Hump Camel function definition\"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    term1 = (4 - 2.1 * x1**2 + (x1**4) / 3) * x1**2\n",
    "    term2 = x1 * x2\n",
    "    term3 = (-4 + 4 * x2**2) * x2**2\n",
    "\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APOSMM Operations\n",
    "\n",
    "APOSMM coordinates multiple local optimization runs starting from a collection of sample points. These local optimization runs occur concurrently, and can incorporate a variety of optimization methods, including from NLopt, PETSc/TAO, and SciPy. Some number of uniformly sampled points is returned by APOSMM for simulation evaluations before local optimization runs can occur, if no prior simulation evaluations are provided. User-requested sample points can also be provided to APOSMM:\n",
    "\n",
    "![6hcsampling](https://raw.githubusercontent.com/Libensemble/libensemble/main/docs/images/sampling_6hc.png)\n",
    "\n",
    "Specifically, APOSMM will begin local optimization runs from those points that don’t have better (more minimal) points nearby within a threshold ``r_k``. For the above example, after APOSMM has returned the uniformly sampled points, for simulation evaluations it will likely begin local optimization runs from the user-requested approximate minima. Providing these isn’t required, but can offer performance benefits.\n",
    "\n",
    "Each local optimization run chooses new points and determines if they’re better by passing them back to be evaluated by the simulation routine. If so, new local optimization runs are started from those points. This continues until each run converges to a minimum:\n",
    "\n",
    "![6hclocalopt](https://raw.githubusercontent.com/Libensemble/libensemble/main/docs/images/localopt_6hc.png)\n",
    "\n",
    "Throughout, generated and evaluated points are appended to the History array, with the field ``'local_pt'`` being ``True`` if the point is part of a local optimization run, and ``'local_min'`` being ``True`` if the point has been ruled a local minimum.\n",
    "\n",
    "## APOSMM Persistence\n",
    "\n",
    "APOSMM uses a Persistent generator. A single worker process initiates APOSMM so that it “persists” and keeps running over the course of the entire libEnsemble routine.\n",
    "\n",
    "APOSMM begins its own concurrent optimization runs, each of which independently produces a sequence of points trying to find a local minimum. These points are given to workers and evaluated by simulation routines.\n",
    "\n",
    "If there are more workers than optimization runs at any iteration of the generator, additional random sample points are generated to keep the workers busy.\n",
    "\n",
    "\n",
    "## Calling Script\n",
    "\n",
    "Start by importing NumPy, libEnsemble routines, APOSMM, our ``sim_f``, and a specialized allocation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from tutorial_six_hump_camel import six_hump_camel\n",
    "\n",
    "from libensemble.libE import libE\n",
    "from libensemble.gen_funcs.persistent_aposmm import aposmm\n",
    "from libensemble.alloc_funcs.persistent_aposmm_alloc import persistent_aposmm_alloc\n",
    "from libensemble.tools import parse_args, add_unique_random_streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allocation function starts a single Persistent APOSMM routine and provides ``sim_f`` output for points requested by APOSMM. Points can be sampled points or points from local optimization runs.\n",
    "\n",
    "APOSMM supports a wide variety of external optimizers. The following statements set optimizer settings to ``'scipy'`` to indicate to APOSMM which optimization method to use, and help prevent unnecessary imports or package installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libensemble.gen_funcs\n",
    "\n",
    "libensemble.gen_funcs.rc.aposmm_optimizers = \"scipy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ``nworkers``, ``libE_specs``, ``sim_specs``, ``gen_specs``, and ``alloc_specs``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers = 4  # One for persistent generator and three for simulations\n",
    "libE_specs = {\"nworkers\": nworkers, \"comms\": \"local\"}\n",
    "\n",
    "sim_specs = {\n",
    "    \"sim_f\": six_hump_camel,  # Simulation function\n",
    "    \"in\": [\"x\"],  # Accepts 'x' values\n",
    "    \"out\": [(\"f\", float)],\n",
    "}  # Returns f(x) values\n",
    "\n",
    "gen_out = [\n",
    "    (\"x\", float, 2),  # Produces 'x' values\n",
    "    (\"x_on_cube\", float, 2),  # 'x' values scaled to unit cube\n",
    "    (\"sim_id\", int),  # Produces sim_id's for History array indexing\n",
    "    (\"local_min\", bool),  # Is a point a local minimum?\n",
    "    (\"local_pt\", bool),  # Is a point from a local opt run?\n",
    "]\n",
    "\n",
    "gen_specs = {\n",
    "    \"gen_f\": aposmm,  # APOSMM generator function\n",
    "    \"persis_in\": [\"f\", \"x\", \"x_on_cube\", \"sim_id\", \"local_min\", \"local_pt\"],  # Fields we want to pass back to APOSMM\n",
    "    \"out\": gen_out,  # Output defined like above dict\n",
    "    \"user\": {\n",
    "        \"initial_sample_size\": 100,  # Random sample 100 points to start\n",
    "        \"localopt_method\": \"scipy_Nelder-Mead\",\n",
    "        \"opt_return_codes\": [0],  # Return code specific to localopt_method\n",
    "        \"max_active_runs\": 6,  # Maximum concurrent local optimization runs\n",
    "        \"lb\": np.array([-2, -1]),  # Lower bound of search domain\n",
    "        \"ub\": np.array([2, 1]),  # Upper bound of search domain\n",
    "    },\n",
    "}\n",
    "\n",
    "alloc_specs = {\"alloc_f\": persistent_aposmm_alloc}\n",
    "\n",
    "exit_criteria = {\"sim_max\": 800}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``gen_specs['user']`` fields above that are required for APOSMM are:\n",
    "\n",
    "* ``'lb'`` - Search domain lower bound\n",
    "* ``'ub'`` - Search domain upper bound\n",
    "* ``'localopt_method'`` - Chosen local optimization method\n",
    "* ``'initial_sample_size'`` - Number of uniformly sampled points generated\n",
    "  before local optimization runs.\n",
    "* ``'opt_return_codes'`` - A list of integers that local optimization\n",
    "  methods return when a minimum is detected. SciPy's Nelder-Mead returns 0,\n",
    "  but other methods (not used in this tutorial) return 1.\n",
    "\n",
    "Also note the following:\n",
    "\n",
    "* ``'x_on_cube'`` in ``gen_specs['out']``. APOSMM works internally on\n",
    "  ``'x'`` values scaled to the unit cube. To avoid back-and-forth scaling\n",
    "  issues, both types of ``'x'``'s are communicated back, even though the\n",
    "  simulation will likely use ``'x'`` values. (APOSMM performs handshake to\n",
    "  ensure that the ``x_on_cube`` that was given to be evaluated is the same\n",
    "  one that is given back.)\n",
    "* ``'sim_id'`` in ``gen_specs['out']``. APOSMM produces points in it's\n",
    "  local History array that it will need to update later, and can best\n",
    "  reference those points (and avoid a search) if APOSMM produces the IDs\n",
    "  itself, instead of libEnsemble.\n",
    "\n",
    "Other options and configurations can be found in the APOSMM [API reference](https://libensemble.readthedocs.io/en/main/examples/aposmm.html).\n",
    "\n",
    "The ``exit_criteria`` tells libEnsemble when to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Ensemble\n",
    "\n",
    "Optionally run the next cell to set up a live graphic of the optimization progress during execution.\n",
    "\n",
    "**WARNING**: The graphic may flicker when the ensemble is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure to view live progress\n",
    "from libensemble.tools.live_data.plot2n import Plot2N\n",
    "\n",
    "libE_specs[\"live_data\"] = Plot2N(plot_type=\"2d\")  # Alt: '3d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set `persis_info` (to provide random seeds to workers) and run the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persis_info = add_unique_random_streams({}, nworkers + 1)\n",
    "\n",
    "H, persis_info, flag = libE(sim_specs, gen_specs, exit_criteria, persis_info, alloc_specs, libE_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minima:\", H[np.where(H[\"local_min\"])][\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Please note that one worker will be “persistent” for APOSMM for the duration of the routine.\n",
    "\n",
    "The output from the above print should resemble:\n",
    "\n",
    "    Minima:[[ 0.08978611 -0.71258202]\n",
    "    [-1.70380373  0.79614422]\n",
    "    [-0.08985348  0.71271365]\n",
    "    [ 1.70352649 -0.79610854]\n",
    "    [ 1.60712922  0.56868726]\n",
    "    [-1.60711706 -0.56868708]\n",
    "    [ 1.70361564 -0.7960268 ]]\n",
    "     \n",
    "The first six values correspond to the local minima for the Six-Hump Camel simulation function.\n",
    "\n",
    "The 7th value is a repeat minimum, as APOSMM will continue to start local optimization runs.\n",
    "\n",
    "Please see the [API reference](https://libensemble.readthedocs.io/en/main/examples/aposmm.html) for more APOSMM configuration options and other information.\n",
    "\n",
    "## Applications\n",
    "\n",
    "APOSMM is not limited to evaluating minima from pure Python simulation functions.\n",
    "Many common libEnsemble use-cases involve using libEnsemble's Executor to launch user\n",
    "applications with parameters requested by APOSMM, then evaluate their output using\n",
    "APOSMM, and repeat until minima are identified. A currently supported example\n",
    "can be found in libEnsemble's [WarpX Scaling Test](https://github.com/Libensemble/libensemble/tree/main/libensemble/tests/scaling_tests/warpx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
